{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ruDALL-E Mass Batcher - Beta",
      "provenance": [],
      "collapsed_sections": [
        "NsyANN6cpSyi",
        "Xs1tP0fUlqPp",
        "kwNBJlTUBZDV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FractalLibrary/ruDALL-E/blob/main/ruDALL_E_Mass_Batcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szVioOX9BlH7"
      },
      "source": [
        "# ruDALL-E Mass Batcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsyANN6cpSyi"
      },
      "source": [
        "## Directory Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQMcsEW2p8qr"
      },
      "source": [
        "#@title Connect Google Drive\n",
        "import os\n",
        "abs_root_path = \"/content\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def ensureProperRootPath():\n",
        "    if len(abs_root_path) > 0:\n",
        "        os.chdir(abs_root_path) # Changes directory to absolute root path\n",
        "        print(\"Root path check: \")\n",
        "        !pwd\n",
        "\n",
        "ensureProperRootPath()\n",
        "\n",
        "folder_name = \"AI_ART\" #@param {type: \"string\"}\n",
        "if folder_name[-1] == '/': #Take care of accidental slashes at the end of a folder name\n",
        "  folder_name = folder_name[:-1]\n",
        "if len(folder_name) > 0:\n",
        "    path_tmp = abs_root_path + \"/drive/MyDrive/\" + folder_name\n",
        "    if not os.path.exists(path_tmp):\n",
        "        os.mkdir(path_tmp)\n",
        "    abs_root_path = path_tmp\n",
        "\n",
        "print(\"Created folder & set root path to: \" + abs_root_path)\n",
        "\n",
        "#@markdown The folder where the images are dumped\n",
        "\n",
        "project_name = \"rudalle\" #@param {type: \"string\"}\n",
        "if project_name[-1] == '/': #Take care of accidental slashes at the end of a folder name\n",
        "  project_name = project_name[:-1]\n",
        "if len(project_name) > 0:\n",
        "      path_tmp = abs_root_path + \"/\" + project_name\n",
        "      if not os.path.exists(path_tmp):\n",
        "          os.mkdir(path_tmp)\n",
        "      abs_root_path = path_tmp\n",
        "print(\"Created project subfolder & set root path to: \" + abs_root_path)\n",
        "\n",
        "ensureProperRootPath()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs1tP0fUlqPp"
      },
      "source": [
        "## Import, install, read in models, etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwzWV3I7SB3g"
      },
      "source": [
        "!nvidia-smi -L\n",
        "!pip install rudalle==0.0.1rc7 > /dev/null\n",
        "\n",
        "from rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_clip\n",
        "from rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan, get_ruclip\n",
        "from rudalle.utils import seed_everything\n",
        "\n",
        "# %%time\n",
        "device = 'cuda'\n",
        "dalle = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device)\n",
        "# %%time\n",
        "try:\n",
        "    realesrgan, tokenizer, ruclip, ruclip_processor\n",
        "except NameError:\n",
        "    realesrgan = get_realesrgan('x4', device=device)\n",
        "    tokenizer = get_tokenizer()\n",
        "    vae = get_vae().to(device)\n",
        "    ruclip, ruclip_processor = get_ruclip('ruclip-vit-base-patch32-v5')\n",
        "    ruclip = ruclip.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwNBJlTUBZDV"
      },
      "source": [
        "## Patches, functions, various things that don't need a huge loading time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkNHZxFGBZDW"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import imageio\n",
        "\n",
        "from rudalle import utils\n",
        "\n",
        "\n",
        "def generate_images(text, tokenizer, dalle, vae, top_k, top_p, images_num, temperature=1.0, bs=8, seed=None,\n",
        "                    use_cache=True):\n",
        "    if seed is not None:\n",
        "        utils.seed_everything(seed)\n",
        "\n",
        "    vocab_size = dalle.get_param('vocab_size')\n",
        "    text_seq_length = dalle.get_param('text_seq_length')\n",
        "    image_seq_length = dalle.get_param('image_seq_length')\n",
        "    total_seq_length = dalle.get_param('total_seq_length')\n",
        "    device = dalle.get_param('device')\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    input_ids = tokenizer.encode_text(text, text_seq_length=text_seq_length)\n",
        "    pil_images, scores = [], []\n",
        "    for chunk in more_itertools.chunked(range(images_num), bs):\n",
        "        chunk_bs = len(chunk)\n",
        "        with torch.no_grad():\n",
        "            attention_mask = torch.tril(torch.ones((chunk_bs, 1, total_seq_length, total_seq_length), device=device))\n",
        "            out = input_ids.unsqueeze(0).repeat(chunk_bs, 1).to(device)\n",
        "            has_cache = False\n",
        "            sample_scores = []\n",
        "            for i in tqdm(range(len(input_ids), total_seq_length)):\n",
        "                logits, has_cache = dalle(out[:, :i], attention_mask,\n",
        "                                          has_cache=has_cache, use_cache=use_cache, return_loss=False)\n",
        "                logits = logits[:, -1, vocab_size:]\n",
        "                logits /= temperature\n",
        "                filtered_logits = transformers.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "                probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "                sample = torch.multinomial(probs, 1)\n",
        "                sample_scores.append(probs[torch.arange(probs.size(0)), sample.transpose(0, 1)])\n",
        "                out = torch.cat((out, sample), dim=-1)\n",
        "            codebooks = out[:, -image_seq_length:]\n",
        "            images = vae.decode(codebooks)\n",
        "            pil_images += utils.torch_tensors_to_pil_list(images)\n",
        "            scores += torch.cat(sample_scores).sum(0).detach().cpu().numpy().tolist()\n",
        "    return pil_images, scores\n",
        "\n",
        "\n",
        "def show(pil_images, nrow=4, filename = None):\n",
        "    imgs = torchvision.utils.make_grid(utils.pil_list_to_torch_tensors(pil_images), nrow=nrow)\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs.cpu()]\n",
        "    fig = None\n",
        "    axs = None\n",
        "    if filename is None:\n",
        "      fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(14, 14))\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = torchvision.transforms.functional.to_pil_image(img)\n",
        "        if filename is not None:\n",
        "          imageio.imwrite(filename, np.array(img))\n",
        "        else:\n",
        "          axs[0, i].imshow(np.asarray(img))\n",
        "          axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    if filename is None:\n",
        "      fig.show()\n",
        "      plt.show()\n",
        "    \n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from rudalle.dalle.utils import divide, split_tensor_along_last_dim\n",
        "\n",
        "\n",
        "@torch.jit.script\n",
        "def gelu_impl(x):\n",
        "    \"\"\"OpenAI's gelu implementation.\"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x * (1.0 + 0.044715 * x * x)))\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return gelu_impl(x)\n",
        "\n",
        "\n",
        "def dalle_layer_forward(self, hidden_states, ltor_mask, has_cache, use_cache):\n",
        "    # hidden_states: [b, s, h]\n",
        "    # ltor_mask: [1, 1, s, s]\n",
        "\n",
        "    # Layer norm at the begining of the transformer layer.\n",
        "    # output = hidden_states\n",
        "    # att_has_cache, mlp_has_cache = True, True\n",
        "    layernorm_output = self.input_layernorm(hidden_states)\n",
        "\n",
        "    # Self attention.\n",
        "    attention_output, att_has_cache = self.attention(\n",
        "        layernorm_output, ltor_mask, has_cache=has_cache, use_cache=use_cache)  # if False else layernorm_output, True\n",
        "\n",
        "    if self.cogview_sandwich_layernorm:\n",
        "        attention_output = self.before_first_addition_layernorm(\n",
        "            attention_output, has_cache=has_cache, use_cache=use_cache)\n",
        "\n",
        "    # Residual connection.\n",
        "    layernorm_input = hidden_states + attention_output\n",
        "\n",
        "    # Layer norm post the self attention.\n",
        "    layernorm_output = self.post_attention_layernorm(\n",
        "        layernorm_input, has_cache=has_cache, use_cache=use_cache)\n",
        "\n",
        "    # MLP.\n",
        "    # mlp_has_cache = True\n",
        "    mlp_output, mlp_has_cache = self.mlp(\n",
        "        layernorm_output, has_cache=has_cache, use_cache=use_cache\n",
        "        )  # if False else layernorm_output, True\n",
        "\n",
        "    if self.cogview_sandwich_layernorm:\n",
        "        mlp_output = self.before_second_addition_layernorm(\n",
        "            mlp_output, has_cache=has_cache, use_cache=use_cache)\n",
        "\n",
        "    # Second residual connection.\n",
        "    output = layernorm_input + mlp_output\n",
        "\n",
        "    return output, att_has_cache and mlp_has_cache\n",
        "\n",
        "\n",
        "# About 1.3x speedup. Query/key/value cat is surprisingly fast.\n",
        "def dalle_sa_forward(self, hidden_states, ltor_mask, has_cache=False, use_cache=False,):\n",
        "    # hidden_states: [b, s, h]\n",
        "    # ltor_mask: [1, 1, s, s]\n",
        "    # Attention heads. [b, s, hp]\n",
        "    \n",
        "    def calculate_attention_scores(query_layer, key_layer, ltor_mask):\n",
        "        key_t = key_layer.transpose(-1, -2)\n",
        "        if self.cogview_pb_relax:\n",
        "            attention_scores = torch.matmul(\n",
        "                query_layer / math.sqrt(self.hidden_size_per_attention_head),\n",
        "                key_t\n",
        "            )\n",
        "        else:\n",
        "            attention_scores = torch.matmul(query_layer, key_t) / math.sqrt(self.hidden_size_per_attention_head)\n",
        "\n",
        "        ltor_mask = ltor_mask[:, :, -attention_scores.shape[-2]:]\n",
        "\n",
        "        attention_scores = torch.mul(attention_scores, ltor_mask) - 10000.0 * (1.0 - ltor_mask)\n",
        "        if self.cogview_pb_relax:\n",
        "            # normalize attention scores. Should not affect resulting softmax value\n",
        "            alpha = 32\n",
        "            attention_scores_scaled = attention_scores / alpha\n",
        "            attention_scores_scaled_maxes, _ = attention_scores_scaled.detach().view(\n",
        "                [attention_scores.size(0), attention_scores.size(1), -1]\n",
        "            ).max(dim=-1)  # max per head per sample\n",
        "            attention_scores_scaled_maxes = attention_scores_scaled_maxes.unsqueeze(-1).unsqueeze(-1).expand(\n",
        "                [-1, -1, attention_scores.size(2), attention_scores.size(3)]\n",
        "            )  # expand to [b, np, s, s]\n",
        "            attention_scores = (attention_scores_scaled - attention_scores_scaled_maxes) * alpha\n",
        "        return attention_scores\n",
        "    \n",
        "    t = hidden_states.shape[-2]\n",
        "    if has_cache and use_cache:\n",
        "        mixed_x_layer = self.query_key_value(hidden_states[:, self.past_output.shape[-2]:, :])\n",
        "    else:\n",
        "        mixed_x_layer = self.query_key_value(hidden_states)\n",
        "\n",
        "    (mixed_query_layer,\n",
        "        mixed_key_layer,\n",
        "        mixed_value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n",
        "\n",
        "    query_layer = self._transpose_for_scores(mixed_query_layer)\n",
        "    key_layer = self._transpose_for_scores(mixed_key_layer)\n",
        "    value_layer = self._transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "    if use_cache and has_cache:\n",
        "        value_layer = torch.cat((self.past_value, value_layer), dim=-2)\n",
        "        key_layer = torch.cat((self.past_key, key_layer), dim=-2)\n",
        "    attention_scores = calculate_attention_scores(\n",
        "        query_layer=query_layer, key_layer=key_layer, ltor_mask=ltor_mask\n",
        "    )\n",
        "\n",
        "    if use_cache:\n",
        "        self.past_key = key_layer\n",
        "        self.past_value = value_layer\n",
        "    else:\n",
        "        has_cache = False\n",
        "\n",
        "    if use_cache and has_cache:\n",
        "        attention_scores = attention_scores[..., -1:, :]\n",
        "    \n",
        "    # Attention probabilities. [b, np, s, s]\n",
        "    attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    # This is actually dropping out entire tokens to attend to, which might\n",
        "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "    attention_probs = self.attention_dropout(attention_probs)\n",
        "\n",
        "    # Context layer.\n",
        "    # [b, np, s, hn]\n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    # [b, s, np, hn]\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size,)\n",
        "    # [b, s, hp]\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "    # Output. [b, s, h]\n",
        "    output = self.dense(context_layer)\n",
        "\n",
        "    # print(output.shape)\n",
        "    if use_cache:\n",
        "        # Can be simplified, but I didn't for readability's sake\n",
        "        if has_cache:\n",
        "            output = torch.cat((self.past_output, output), dim=-2)\n",
        "            self.past_output = output\n",
        "        else:\n",
        "            self.past_output = output\n",
        "        has_cache = True \n",
        "    output = self.output_dropout(output)\n",
        "    return output, has_cache\n",
        "\n",
        "\n",
        "def dalle_mlp_forward(self, hidden_states, has_cache=False, use_cache=False):\n",
        "    if has_cache and use_cache:\n",
        "        hidden_states = hidden_states[:, self.past_x.shape[1]:]\n",
        "\n",
        "    # [b, s, 4hp]\n",
        "    x = self.dense_h_to_4h(hidden_states)\n",
        "    x = gelu(x)\n",
        "    # [b, s, h]\n",
        "    x = self.dense_4h_to_h(x)\n",
        "    if use_cache:\n",
        "        # Can be simplified, but isn't for readability's sake\n",
        "        if has_cache:\n",
        "            x = torch.cat((self.past_x, x), dim=-2)\n",
        "            self.past_x = x\n",
        "        else:\n",
        "            self.past_x = x\n",
        "        has_cache = True\n",
        "    else:\n",
        "        has_cache = False\n",
        "    output = self.dropout(x)\n",
        "    return output, has_cache\n",
        "\n",
        "\n",
        "# Speeds up like 6 seconds.\n",
        "def ln_forward(self, input, has_cache=False, use_cache=False):\n",
        "    if has_cache and use_cache:\n",
        "        input = input[:, self.past_output.shape[1]:]\n",
        "    \n",
        "    output = F.layer_norm(\n",
        "        input, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "    \n",
        "    if use_cache:\n",
        "        # Can be simplified, but isn't readability's sake\n",
        "        if has_cache:\n",
        "            output = torch.cat((self.past_output, output), dim=1)\n",
        "            self.past_output = output\n",
        "        else:\n",
        "            self.past_output = output\n",
        "        has_cache = True\n",
        "    else:\n",
        "        has_cache = False\n",
        "    return output\n",
        "\n",
        "\n",
        "import inspect\n",
        "from functools import partial\n",
        "\n",
        "for layer in dalle.module.transformer.layers:\n",
        "    layer.forward = partial(dalle_layer_forward, layer)\n",
        "    layer.mlp.forward = partial(dalle_mlp_forward, layer.mlp)\n",
        "    layer.attention.past_attentions = None\n",
        "    layer.attention.past_query = None\n",
        "    layer.attention.forward = partial(dalle_sa_forward, layer.attention)\n",
        "    for ln in [layer.input_layernorm,\n",
        "               layer.before_first_addition_layernorm,\n",
        "               layer.post_attention_layernorm,\n",
        "               layer.before_second_addition_layernorm]:\n",
        "        ln.forward = partial(ln_forward, ln)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN5z2OS1SYue"
      },
      "source": [
        "## Run things"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ruDALLE generation\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#@markdown Use Google Translate to get a prompt in Russian.\n",
        "text = '\\u0436\\u0438\\u0440\\u0430\\u0444 \\u043D\\u0430 \\u043F\\u0430\\u0440\\u043E\\u0432\\u043E\\u0439 \\u0442\\u044F\\u0433\\u0435' #@param {type:\"string\"}\n",
        "\n",
        "random_seed =  42#@param {type:\"integer\"}\n",
        "seed_everything(random_seed)\n",
        "\n",
        "#@markdown Total number of images produced is images_per_cycle * num_cycles.\n",
        "#@markdown For free tier, do not exceed 3 images per cycle.\n",
        "images_per_cycle =  3#@param {type:\"integer\"}\n",
        "num_cycles = 20#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown top_k_ and top_p_ control the number of tokens to be considered by probability and count.\n",
        "#@markdown It's recommended that you use the defaults here\n",
        "top_k_ =  1024#@param {type:\"integer\"}\n",
        "top_p_ =  .99#@param {type:\"number\"}\n",
        "hash_val = str(hash(text + str(top_k_) + str(top_p_)))[-5:]\n",
        "\n",
        "imageIndex = 0\n",
        "for top_k, top_p, images_num in tqdm([(top_k_, top_p_, images_per_cycle),]*num_cycles):\n",
        "    pil_images = []\n",
        "    scores = []\n",
        "    _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, top_p=top_p)\n",
        "    show([pil_image for pil_image, score in sorted(zip(_pil_images, _scores), key=lambda x: -x[1])], num_cycles * images_per_cycle)\n",
        "    pil_images += _pil_images\n",
        "    scores += _scores\n",
        "\n",
        "    for i in range(images_per_cycle):\n",
        "      sr_images = super_resolution([pil_images[i]], realesrgan)\n",
        "      filename = abs_root_path + f\"/{hash_val}-{random_seed}{imageIndex:04}.png\"\n",
        "      imageIndex += 1\n",
        "      show(sr_images, 1, filename = filename)"
      ],
      "metadata": {
        "id": "yix72oNl0aPu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fBPK2Lx96Ei",
        "cellView": "form"
      },
      "source": [
        "#@title ruDALLE generation - Queue\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#@markdown If you give the next input you want here and run the code, it'll \n",
        "#@markdown kick off the moment the previous one ends.  Then you can have this \n",
        "#@markdown running all the time, constantly, FOREVER!!!!!!\n",
        "\n",
        "#@markdown Use Google Translate to get a prompt in Russian.\n",
        "text = '\\u044D\\u043C\\u043E \\u0432\\u0430\\u043C\\u043F\\u0438\\u0440' #@param {type:\"string\"}\n",
        "\n",
        "random_seed =  42#@param {type:\"integer\"}\n",
        "seed_everything(random_seed)\n",
        "\n",
        "#@markdown Total number of images produced is images_per_cycle * num_cycles.\n",
        "#@markdown For free tier, do not exceed 3 images per cycle.\n",
        "images_per_cycle = 1 #@param {type:\"integer\"}\n",
        "num_cycles = 20#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown top_k_ and top_p_ control the number of tokens to be considered by probability and count.\n",
        "#@markdown It's recommended that you use the defaults here\n",
        "top_k_ =  1024#@param {type:\"integer\"}\n",
        "top_p_ =  .99#@param {type:\"number\"}\n",
        "hash_val = str(hash(text + str(top_k_) + str(top_p_)))[-5:]\n",
        "\n",
        "imageIndex = 0\n",
        "for top_k, top_p, images_num in tqdm([(top_k_, top_p_, images_per_cycle),]*num_cycles):\n",
        "    pil_images = []\n",
        "    scores = []\n",
        "    _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, top_p=top_p)\n",
        "    show([pil_image for pil_image, score in sorted(zip(_pil_images, _scores), key=lambda x: -x[1])], num_cycles * images_per_cycle)\n",
        "    pil_images += _pil_images\n",
        "    scores += _scores\n",
        "\n",
        "    for i in range(images_per_cycle):\n",
        "      sr_images = super_resolution([pil_images[i]], realesrgan)\n",
        "      filename = abs_root_path + f\"/{hash_val}-{random_seed}{imageIndex:04}.png\"\n",
        "      imageIndex += 1\n",
        "      show(sr_images, 1, filename = filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQOeuIpSdsHO"
      },
      "source": [
        "Fin"
      ]
    }
  ]
}