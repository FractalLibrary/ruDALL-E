{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FractalLibrary/ruDALL-E/blob/main/ruDALL_E_Arbitrary_Resolution_Mass_Batcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBmp5-LlH2R1"
      },
      "source": [
        "# ruDALLE arbitrary resolution\n",
        "\n",
        "by @nev#4905 and u/Chordus"
      ],
      "id": "hBmp5-LlH2R1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe29PRif9VNb"
      },
      "source": [
        "## Garbage Collect as necessary\n",
        "##### (Shouldn't be necessary any more, but kept just in case)"
      ],
      "id": "Fe29PRif9VNb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIB50jsN9clV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "id": "kIB50jsN9clV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL9-zIsgH3A8"
      },
      "source": [
        "## install dependencies"
      ],
      "id": "ZL9-zIsgH3A8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdac3238"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/neverix/ru-dalle\n",
        "!mv -f ru-dalle/* ru-dalle/.git .\n",
        "!rm -rf ru-dalle\n",
        "!git checkout better-caching\n",
        "!pip install -e ."
      ],
      "id": "fdac3238"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9264c4b3"
      },
      "outputs": [],
      "source": [
        "from rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_clip\n",
        "from rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan, get_ruclip\n",
        "from rudalle.utils import seed_everything\n",
        "\n",
        "import torch\n",
        "import gc"
      ],
      "id": "9264c4b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c76c31da"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "# device = \"cpu\"\n",
        "tokenizer = get_tokenizer()\n",
        "dalle = get_rudalle_model('Malevich', pretrained=True,\n",
        "                           fp16=device == \"cuda\",\n",
        "                           device=device\n",
        "                          )"
      ],
      "id": "c76c31da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f010cf3c"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    realesrgan\n",
        "except NameError:\n",
        "    realesrgan = get_realesrgan('x4', device=device)\n",
        "\n",
        "# realesrgan = get_realesrgan('x4', device=device)\n",
        "vae = get_vae().to(device)\n",
        "# ruclip, ruclip_processor = get_ruclip('ruclip-vit-base-patch32-v5')\n",
        "# ruclip = ruclip.to(device)"
      ],
      "id": "f010cf3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkZwdZhwAx1i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "id": "CkZwdZhwAx1i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV1DNMBKzvQM"
      },
      "source": [
        "## code"
      ],
      "id": "hV1DNMBKzvQM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXPIt1k3xmS-"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from os.path import join\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import transformers\n",
        "import more_itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from rudalle import utils\n",
        "\n",
        "\n",
        "def generate_images(text, tokenizer, dalle, vae, top_k, top_p, images_num, image_prompts=None, temperature=1.0, bs=8,\n",
        "                    seed=None, use_cache=True, w=32, h=48):\n",
        "    # TODO docstring\n",
        "    if seed is not None:\n",
        "        utils.seed_everything(seed)\n",
        "    vocab_size = dalle.get_param('vocab_size')\n",
        "    text_seq_length = dalle.get_param('text_seq_length')\n",
        "    image_seq_length = dalle.get_param('image_seq_length')\n",
        "    total_seq_length = dalle.get_param('total_seq_length')\n",
        "    device = dalle.get_param('device')\n",
        "    real = 32\n",
        "\n",
        "    text = text.lower().strip()\n",
        "    input_ids = tokenizer.encode_text(text, text_seq_length=text_seq_length)\n",
        "    pil_images, scores = [], []\n",
        "    cache = None\n",
        "    past_cache = None\n",
        "    try:\n",
        "        for chunk in more_itertools.chunked(range(images_num), bs):\n",
        "            chunk_bs = len(chunk)\n",
        "            with torch.no_grad():\n",
        "                attention_mask = torch.tril(torch.ones((chunk_bs, 1, total_seq_length, total_seq_length), device=device))\n",
        "                out = input_ids.unsqueeze(0).repeat(chunk_bs, 1).to(device)\n",
        "                grid = torch.zeros((h, w)).long().cuda()\n",
        "                has_cache = False\n",
        "                sample_scores = []\n",
        "                if image_prompts is not None:\n",
        "                    prompts_idx, prompts = image_prompts.image_prompts_idx, image_prompts.image_prompts\n",
        "                    prompts = prompts.repeat(chunk_bs, 1)\n",
        "                for idx in tqdm(range(out.shape[1], total_seq_length-real*real+w*h)):\n",
        "                    idx -= text_seq_length\n",
        "                    if image_prompts is not None and idx in prompts_idx:\n",
        "                        out = torch.cat((out, prompts[:, idx].unsqueeze(1)), dim=-1)\n",
        "                    else:\n",
        "                        y = idx // w\n",
        "                        x = idx % w\n",
        "                        x_from = max(0, min(w-real, x-real//2))\n",
        "                        y_from = max(0, y-real//2)\n",
        "                        # print(y, y_from, x, x_from, idx, w, h)\n",
        "                        outs = []\n",
        "                        xs = []\n",
        "                        for row in range(y_from, y):\n",
        "                            for col in range(x_from, min(w, x_from+real)):\n",
        "                                outs.append(grid[row, col].item())\n",
        "                                xs.append((row, col))\n",
        "                        for col in range(x_from, x):\n",
        "                            outs.append(grid[y, col].item())\n",
        "                            xs.append((y, col))\n",
        "                        rev_xs = {v: k for k, v in enumerate(xs)}\n",
        "                        if past_cache is not None:\n",
        "                            cache = list(map(list, cache.values()))\n",
        "                            rev_past = {v: k for k, v in enumerate(past_cache)}\n",
        "                            for i, e in enumerate(cache):\n",
        "                                for j, c in enumerate(e):\n",
        "                                    t = cache[i][j]\n",
        "                                    t, c = t[..., :text_seq_length, :], t[..., text_seq_length:, :]\n",
        "                                    # nc = []\n",
        "                                    # for l, m in xs:\n",
        "                                    #     while (l, m) not in rev_past:\n",
        "                                    #         break  # will pass\n",
        "                                    #         if l <= 0 and m <= 0:\n",
        "                                    #             break\n",
        "                                    #         m -= 1\n",
        "                                    #         if m < 0:\n",
        "                                    #             l -= 1\n",
        "                                    #             m = real - 1\n",
        "                                    #     if (l, m) not in rev_past:\n",
        "                                    #         break\n",
        "                                    #     nc.append(c[..., rev_past[l, m], :])\n",
        "                                    # if nc:\n",
        "                                    #     c = torch.stack(nc, dim=-2)\n",
        "                                    #     # print(c.shape, t.shape, nc[0].shape)\n",
        "                                    #     t = torch.cat((t, c), dim=-2)\n",
        "                                    cache[i][j] = t\n",
        "                            cache = dict(zip(range(len(cache)), cache))\n",
        "                        past_cache = xs\n",
        "                        logits, cache = dalle(torch.cat((input_ids.to(device).ravel(),\n",
        "                                                            torch.from_numpy(np.asarray(outs)).long().to(device)),\n",
        "                                                            dim=0).unsqueeze(0), attention_mask,\n",
        "                                                cache=cache, use_cache=True, return_loss=False)\n",
        "                        # logits = logits[:, -1, vocab_size:]\n",
        "                        logits = logits[:, :, vocab_size:].view((-1, logits.shape[-1] - vocab_size))\n",
        "                        logits /= temperature\n",
        "                        filtered_logits = transformers.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "                        probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
        "                        sample = torch.multinomial(probs, 1)\n",
        "                        sample_scores.append(probs[torch.arange(probs.size(0)), sample.transpose(0, 1)])\n",
        "                        # out = torch.cat((out, sample), dim=-1)\n",
        "                        sample, xs = sample[-1:], xs[-1:]\n",
        "                        # print(sample.item())\n",
        "                        grid[y, x] = sample.item()\n",
        "                        # for s, (y, x) in zip(sample, xs):\n",
        "                            # i = y * w + x\n",
        "                            # i += 1\n",
        "                            # grid[i // w, i % w] = s.item()\n",
        "                        codebooks = grid.flatten().unsqueeze(0)\n",
        "                        # print(codebooks.shape)\n",
        "                        images = vae.decode(codebooks)\n",
        "                        pil_images += utils.torch_tensors_to_pil_list(images)\n",
        "                        # show(utils.torch_tensors_to_pil_list(images))\n",
        "                # codebooks = out[:, -image_seq_length:]\n",
        "                # codebooks = grid.flatten().unsqueeze(0)\n",
        "                # images = vae.decode(codebooks)\n",
        "                # pil_images += utils.torch_tensors_to_pil_list(images)\n",
        "                # scores += torch.cat(sample_scores).sum(0).detach().cpu().numpy().tolist()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        pass\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    return pil_images, scores\n",
        "\n",
        "#@title adapt the vqgan decoder to a new non-square resolution. uses the global `h` \n",
        "from math import sqrt, log\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import einsum\n",
        "from einops import rearrange\n",
        "from taming.modules.diffusionmodules.model import Encoder, Decoder\n",
        "\n",
        "from functools import partial\n",
        "def decode(self, img_seq):\n",
        "    b, n = img_seq.shape\n",
        "    one_hot_indices = torch.nn.functional.one_hot(img_seq, num_classes=self.num_tokens).float()\n",
        "    z = (one_hot_indices @ self.model.quantize.embed.weight)\n",
        "    z = rearrange(z, 'b (h w) c -> b c h w', h=h\n",
        "                  # int(sqrt(n))\n",
        "                  )\n",
        "    img = self.model.decode(z)\n",
        "    img = (img.clamp(-1., 1.) + 1) * 0.5\n",
        "    return img\n",
        "vae.decode = partial(decode, vae)\n"
      ],
      "id": "lXPIt1k3xmS-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_TwEF23GU9i"
      },
      "source": [
        "## Directory Setup"
      ],
      "id": "3_TwEF23GU9i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCudd4XEGYHe"
      },
      "outputs": [],
      "source": [
        "#@title Connect Google Drive\n",
        "import os\n",
        "drive_path = \"/content\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def ensureProperRootPath():\n",
        "    if len(drive_path) > 0:\n",
        "        os.chdir(drive_path) # Changes directory to absolute root path\n",
        "        print(\"Root path check: \")\n",
        "        !pwd\n",
        "\n",
        "ensureProperRootPath()\n",
        "\n",
        "folder_name = \"AI_ART\" #@param {type: \"string\"}\n",
        "if folder_name[-1] == '/': #Take care of accidental slashes at the end of a folder name\n",
        "  folder_name = folder_name[:-1]\n",
        "if len(folder_name) > 0:\n",
        "    path_tmp = drive_path + \"/drive/MyDrive/\" + folder_name\n",
        "    if not os.path.exists(path_tmp):\n",
        "        os.mkdir(path_tmp)\n",
        "    drive_path = path_tmp\n",
        "\n",
        "print(\"Created folder & set root path to: \" + drive_path)\n",
        "\n",
        "#@markdown The folder where the images are dumped\n",
        "\n",
        "project_name = \"rudalle-arb\" #@param {type: \"string\"}\n",
        "if project_name[-1] == '/': #Take care of accidental slashes at the end of a folder name\n",
        "  project_name = project_name[:-1]\n",
        "if len(project_name) > 0:\n",
        "      path_tmp = drive_path + \"/\" + project_name\n",
        "      if not os.path.exists(path_tmp):\n",
        "          os.mkdir(path_tmp)\n",
        "      drive_path = path_tmp\n",
        "print(\"Created project subfolder & set root path to: \" + drive_path)\n",
        "\n",
        "ensureProperRootPath()"
      ],
      "id": "JCudd4XEGYHe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dea7796"
      },
      "source": [
        "## generation by ruDALLE"
      ],
      "id": "0dea7796"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3a4bf4b5"
      },
      "outputs": [],
      "source": [
        "#@markdown settings\\\n",
        "#@markdown random seed (set to positive to use)\n",
        "seed =  42#@param {type: \"integer\"}\n",
        "#@markdown text prompt (russian)\n",
        "text = '\\u041B\\u0430\\u0432\\u043A\\u0440\\u0430\\u0444\\u0442\\u043E\\u0432\\u0441\\u043A\\u0438\\u0445 \\u0440\\u0430\\u0434\\u0443\\u0433\\u0430'  #@param {type: \"string\"}\n",
        "\n",
        "#@markdown - Widths lower than 256 will be set to 256, as ruDALL-E cannot handle smaller widths\\\n",
        "#@markdown - Images that are too large are likely to crash, and images around that threshhold may result in a crash on the second run.  Fix unknown.\n",
        "#@markdown - 512x512 images can be rendered en masse\n",
        "#@markdown - 1024x288 images will crash on the second run\n",
        "\n",
        "w =   1024#@param {type: \"number\"}\n",
        "h =   288#@param {type: \"number\"}\n",
        "#@markdown Dimensions will be rounded to units of 8 pixels\n",
        "\n",
        "if w < 256:\n",
        "  w = 256\n",
        "if h < 8:\n",
        "  h = 256\n",
        "\n",
        "w = int(w/8)\n",
        "h = int(h/8)\n",
        "\n",
        "if seed > 0:\n",
        "    seed_everything(seed)\n",
        "\n",
        "num_renders =   10#@param {type: \"number\"}\n",
        "\n",
        "#@markdown Image quality/match.  It's recommended that you use the defaults here\n",
        "top_k_ =  1024#@param {type:\"integer\"}\n",
        "top_p_ =  .99#@param {type:\"number\"}\n",
        "hash_val = str(hash(text + str(w) + str(h) + str(top_k_) + str(top_p_)))[-5:]\n",
        "print('File prefix:', hash_val)\n",
        "\n",
        "pil_images = []\n",
        "scores = []\n",
        "\n",
        "for i in range(num_renders):\n",
        "  for top_k, top_p, images_num in [\n",
        "    (top_k_, top_p_, 1), \n",
        "  ]:\n",
        "    images_num = 1\n",
        "    _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, top_p=top_p,\n",
        "                                            h=h, w=w, use_cache=False)\n",
        "    pil_images += _pil_images\n",
        "    scores += _scores\n",
        "\n",
        "  pil_images[-1].save(f\"/sample{hash_val}-{seed}{i:03}.png\")\n",
        "  pil_images[-1]\n",
        "\n",
        "  sr_images = super_resolution([pil_images[-1]], realesrgan)\n",
        "  filename = drive_path + f\"/{hash_val}-{seed}{i:03}.png\"\n",
        "  sr_images[-1].save(filename)\n",
        "\n",
        "  pil_images = []\n",
        "  sr_images = []\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "id": "3a4bf4b5"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Fe29PRif9VNb",
        "ZL9-zIsgH3A8",
        "hV1DNMBKzvQM",
        "3_TwEF23GU9i"
      ],
      "name": "ruDALL-E Arbitrary Resolution - Mass Batcher",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}